---
title: "Lab 4"
author: "Shan He, Joanna Huang, Tiffany Jaya"
date: "17 December 2017"
output: pdf_document
---

```{r echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(car) # scatterplotMatrix
library(ggplot2) # ggplot
library(moments) # skewness
library(outliers) # outlier
library(tidyr) # gather
library(stargazer) # stargazer for regression table
library(lmtest) # coefficient hypothesis test 
library(sandwich) # vcovHC heteroskedasticity-robust standard error
# prevent scientific notation, specifically used in outliers package
options(scipen=999)
```

# Introduction

The purpose of this report is to generate policy suggestions based on our understanding of the determinants of crime in North Carolina in 1987. We will list out the limitations of our analysis, including any estimates that suffer from endogeneity bias.

# Exploratory Data Analysis

```{r}
# load the data
data <- read.csv("crime.csv")
# verify that it only contains data from 1987
unique(data$year)
# list number of counties
length(unique(data$county))
# list number of western, central, and urban counties
c(sum(data$west == 1), sum(data$central == 1), sum(data$urban == 1))
# list number of western & urban counties and central & urban counties
c(sum(data$west == 1 & data$urban == 1), sum(data$central == 1 & data$urban == 1))
# verify number of missing values
colSums(sapply(data, is.na))
```

The dataset contains 90 counties from North Carolina, all of which is collected in 1987. Out of the 90 counties, 21 are from western NC (out of which 1 is also urban), 34 are from central NC (out of which 5 is also urban), and 8 are considered urban counties. There are no missing values which will make our analysis easier.

Perusing the variables, we note that there are
For now, we take note on variables with probabilities and percentages that fall outside the range. Our assumption is that probabilities are in the range [0, 1] and percentages are in the range [0, 100].

For now, we will not take into consideration probabilities that are greater than 1 or less than 0 as well as percentages that are greater than 1 or less than 0. The assumption is that probabilities are in the range [0, 1] and percentages are in the range [0, 100]. Until we know the reason why the values are outside their range, we will not employ datapoints that do not conform to this assumption.

```{r}
# list number of probabilities (prbarr, prbconv, prbpris, mix) that are not in range [0, 1]
c(sum(data$prbarr < 0 | 1 < data$prbarr), sum(data$prbconv < 0 | 1 < data$prbconv),
  sum(data$prbpris < 0 | 1 < data$prbpris), sum(data$mix < 0 | 1 < data$mix))
# list number of percentages (pctymle, pctmin80) that are not in range [0, 100]
c(sum(data$pctymle < 0 | 100 < data$pctymle), sum(data$pctmin80 < 0 | 100 < data$pctmin80))
```

$prbarr$ and $prbconv$ contain 1 and 10 datapoints respectively that do not conform to the probability assumption.

We then plot each numeric variable in a histogram to see its sample distribution.

```{r message=FALSE}
# plot every variable except X, county, year, west, central, urban
num.data <- data[!(names(data) %in% c("X", "county", "year", "west", "central", "urban"))]
ggplot(gather(num.data), aes(value)) +
       facet_wrap(~key, scales="free") +
       geom_histogram()
```

```{r}
skewness(num.data)
```

Most of the sample distributions appear to be positively skewed. We will take into consideration of logarithmic transformations, depending whether the interpretations make sense, when it is time to include the variables into the regression model.

From the histograms, we also see several notable outliers. We are under the impression that a county which has outlier in one variable will likely have outlier in another variable. For this reason, we have listed counties which have repeated outliers when we iterate through the entire numeric variables.

```{r}
# iterate through each numeric variable and list the outlier counties and their respective frequency
county.ids <- c()
for(var in num.data) {
  var.out <- boxplot.stats(var)$out
  county.ids <- c(county.ids, data[var %in% var.out, ]$county)
}
table(county.ids)
```
```{r}
# list the most extreme outlier
outlier(num.data)
```

One outlier that is interesting to note is that the weekly wage in the service industry for county with id 185 is \$2177.10, which is approximately eight times higher than the median. We do not know if the value is inputted incorrectly or if the county in general is making a weekly wage of \$2177.10 in the service industry.

```{r}
summary(data$wser)
```

# Research Question

James Q. Wilson and George Kelling's "broken windows theory" in 1982 led to a nation-wide movement for stricter crime-fighting policies between the 1980s and 1990s. The theory states:  

> *if the first broken window in a building is not repaired, then people*
> *who like breaking windows will assume that no one cares about the*
> *building and more windows will be broken. Soon the building will have*
> *no windows....*

The belief was that by adopting a zero tolerance approach that enforced even the lowest level offenses, crime rates would subsequently go down. While New York City notably enforced this more stringent approach, San Francisco went the opposite direction of less strident law enforcement policies that reduced arrests, prosecutions and incarceration rates. Both sides experienced considerable declines in crime rates. Thus we hope to test the "broken windows theory" for the counties of South Carolina in 1987 and answer the question: Does the conservative approach of deterrence through arrests, incapacitation through imprisonment, harsh sentencing and higher police per capita lead to lower crime rates?

# Model 1: only the explanatory variables of key interest

Based on the research question, our initial proposed model will include $crmrte$ as the dependent variable and all variables related to stricter law enforcement policies: $prbarr$, $prbconv$, $prbpris$, $avgsen$, and $polpc$ as independent variables. Assuming the "broken windows theory" is valid, we expect generally negative coefficients for all variables.

Given that the histogram of $polpc$ has a significant positive skew, we noted that it would do well to have a log transformation applied to $polpc$ since its values are non-zero and positive. The same can be said about the dependent variable $crmrte$ where its histogram is positively skewed and its values are non-zero and positive.

```{r out.width="49%", fig.align="center", fig.show="hold"}
# before and after log transformation
hist(data$polpc); hist(log(data$polpc))
hist(data$crmrte); hist(log(data$crmrte))
```

Although the marginal distributions of $prbarr$, $prbconv$, and $prbpris$ could benefit from a logit transformation, applying the transformation will make the residuals less normally distributed as seen from the QQ plots for their residuals. **[COMMENT] This arguement is not very strong, and if we comment on how logit tranformation can help, we should show proofs. Jennifer said not to conclude things that aren't in the report.** We steered away from taking the log of these variables because it can make the values more extreme. For this reason, we kept $prbarr$, $prbconv$, and $prbpris$ as is for the linear regression model.

```{r out.width="49%", fig.align="center", fig.show="hold"}
# comparing with and without logit transformation on probability variables
plot(lm(log(crmrte) ~ logit(prbarr) + logit(prbconv) + logit(prbpris) + avgsen + log(polpc), data=data),
     which=2, main="with logit\n")
plot(lm(log(crmrte) ~ prbarr + prbconv + prbpris + avgsen + log(polpc), data=data),
     which=2, main="without logit\n")
```

We also kept $avgsen$ as is for easier interpretation. Plus, applying the log on $avgsen$ does not seem to affect the normality of the residual distribution.

Next, we want to check the relationships between the chosen independent variables and our dependent variable, before and after transformations.

```{r}
scatterplotMatrix(~ crmrte + log(crmrte) + prbarr + prbconv + prbpris + avgsen + polpc + log(polpc), data = data)
```

As we can see from the scatterplot matrix, none of the transformations caused any non-linear relationship.

Hence, we propose our first model as follows which contains all explanatory variables of key interest:

$$ log(crmrte) = \beta_0 + \beta_1 \cdot prbarr + \beta_2 \cdot prbconv + \beta_3 \cdot prbpris + \beta_4 \cdot avgsen + \beta_5 \cdot log(polpc) + u $$

We will now run the model and test the validity of the 6 CLM assumptions:

```{r}
m1 <- lm(log(crmrte) ~ prbarr + prbconv + prbpris + avgsen + log(polpc), data=data)
```

## Outliers in Model 1

```{r}
plot(m1, which=5)
```

Looking at the residuals vs. leverage plot, it appears there are a couple outliers that have considerable leverage on the regression, but all of them have less than 0.5 Cook's distance, showing little influence.

```{r}
data[data$X == 51 | data$X == 79,]
```

Upon closer inspection, the outlier, record 51, has percentages above 100% for both arrest and conviction. Since this is not actually possible, we should consider removing this outlier. **[COMMENT] we should make a decision on whether to remove them or not, they don't have much influence in model 1 though **

## CLM 1 - A linear model

The model is specified such that the dependent variable is a linear function of the explanatory variables. As shown in the scatterplot matrix above, all of the dependent variables in the model seem to have a linear relationship with the independent variable $log(crmrte)$

## CLM 2 - Random Sampling

We do not know how the survey is collected. We assume that the variables are representative of the entire population distribution, but we cannot assess this assumption perfectly. Since there is a possibility that the individuals who collect the survey reach out to only one municipal police department instead of the county police department, the data collected this way are not representative of the county. There is nothing we can do to correct this, so we note this as a potential weakness in the analysis.

## CLM 3 - Multicollinearity

As a quick test of the multicollinearity condition, we check the correlation of the explanatory variables and their Variance Inflation Factors (VIF):

```{r}
# correlation matrix of explanatory variables
data$log.polpc <- log(data$polpc)
cor(data.matrix(subset(data, select=c("prbarr", "prbconv", "prbpris", "avgsen", "polpc", "log.polpc"))))
# verify VIFs are less than 10
vif(m1)
```

The explanatory variables (prbarr, prbconv, polpc, logavgsen and prbpris) are not perfectly correlated and the VIFs are low (i.e. less than 10), so there is no perfect multicollinearity of the independent variables.

Is the assumption valid? **Yes**
**Response**: No response required.

## CLM 4 – Zero-Conditional Mean
To see whether there is a zero-conditional mean across all x’s, we will plot the residuals against the fitted values.

```{r}
plot(m1, which=1)
```

The plot indicates little evidence that the zero-conditional mean assumption doesn’t hold, as the red spline line remains close to zero despite its slight dip and rise at both ends due to less observations.

```{r}
(cov(data$prbarr,m1$residuals))
(cov(data$prbconv,m1$residuals))
(cov(data$avgsen,m1$residuals))
(cov(log(data$polpc),m1$residuals))
(cov(data$prbpris,m1$residuals))
```

The covariances of the three independent variables with the residuals are very close to zero indicating they are likely exogenous.

Is the assumption valid? **Yes**

**Response**: Not Needed

## CLM 5 - Homoscedasticity
To determine whether the variance of $\mu$ is fixed for all x’s, we will first take a look at the residuals plotted against the fitted values to see whether the variance of residuals is constant across the fitted values.  
```{r}
plot(m1, which=c(1,3))
```
The plots indicate no strong evidence of heteroskedasticity.

To further understand whether the model meets homoskedasticity, we will perform statistical tests Breusch-Pagan and the Score-test for non-constant error variance.
```{r, warning = FALSE}
library(lmtest)
bptest(m1)
```
With a p-value of 0.049, we cannot reject the null hypothesis of homoskedasticity at the 5% significance level.

```{r}
ncvTest(m1)
```

With a p-value of .057, we cannot reject the hypothesis of constant error variance.

Is the assumption valid? **Yes**
**Response**: Though we can assume homoskedasticity, we will move forward with robust standard errors, as they provide more accurate p-values.

##CLM 6 – Normality of residuals
To determine whether there is normality of the residuals, we will use a histogram or Q-Q plots of the residuals and visually observe whether there is normality.

```{r}
# normality of standard residuals
rstnd = rstandard(m1)
hist(rstnd, main="Histogram standard residuals", breaks = 50, freq=FALSE)
curve(dnorm(x, mean=0, sd=sd(rstnd)), col="red", lwd=2, add=TRUE)
```
```{r}
# normality of studentized residuals
stud = rstudent(m1)
hist(stud, main="Histogram studentized residuals", breaks = 50, freq=FALSE)
curve(dnorm(x, mean=0, sd=1), col="red", lwd=2, add=TRUE)
```

```{r}
# Q-Q plot standard residuals
qqPlot(rstnd, distribution="norm", pch=20, main="QQ-Plot standard residuals")
qqline(rstnd, col="red", lwd=2)
```

```{r}
# Q-Q plot studentized residuals
qqPlot(stud, distribution="norm", pch=20, main="QQ-Plot studentized residuals")
qqline(stud, col="red", lwd=2)
```
The histograms of the residuals have a negative skew, and QQ-plots further demonstrate nonormality in the error distribution.

```{r}
#check sample size for model 1
nobs(m1)
```

Is the assumption valid? **No**
Response: Since we have 90 observations in model 1, we should be confident to invoke CLM and assume normality of the sampling distribution of the slope parameters when doing hypothesis tests.

# Model 2
One model that includes key explanatory variables and only covariates that you believe increase the
accuracy of your results without introducing bias (for example, you should not include outcome variables
that will absorb some of the causal effect you are interested in). This model should strike a balance
between accuracy and parsimony and reflect your best understanding of the determinants of crime. 

=======
# Model 3: most, if not all, other covariates

Adding arbitrary variables uses up valuable degrees of freedom which increases the standard error which in turn reduces the precision of this estimate. This effect is especially pronounced in small sample size. 

One model that includes the previous covariates, and most, if not all, other covariates. A key purpose
of this model is to demonstrate the robustness of your results to model specification.

```{r}
# EDA for variables that shouldn't be included
unique(cbind(data$west, data$central))
```
Note that although we don't have any counties that are both in west and central, as expected, we see some counties that neither in west or central. In order to consider the effect on different regions, we will need to use both indicator variables in our model. 

```{r}
summary(data[(names(data) %in% c("wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc", "pctmin80", "mix", "pctymle"))])
```

```{r}
#check for any non-linear relationship between variables and dependent variable
scatterplotMatrix(~ log(crmrte) + wcon + wtuc + wtrd + wfir + wser, data = data)
scatterplotMatrix(~ log(crmrte) + wmfg + wfed + wsta + wloc, data = data)
scatterplotMatrix(~ log(crmrte) + pctmin80 + mix + pctymle, data = data)
```

None of the variables shows strong evidence of non-linear relationship with the dependent variable $log(crmrte)$. 

```{r}
for(col in names(data[(names(data) %in% c("wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc", "pctmin80", "mix", "pctymle"))]))
{
  hist(data[,col], breaks = 50, main = paste("Histogram of ", col), xlab = col)
}
```

We notice that the distributions for the variables $wtrd$, $wfir$, $wser$, $wmfg$, $wloc$, $mix$ are positively skewed, since they are all postive values, we can apply log transformations to all of them. For the ease of the intepretation of the model, we apply log tranformation to the other wage related variables as well.

```{r}
for(col in names(data[(names(data) %in% c("wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", "wsta", "wloc", "mix"))]))
{
  hist(log(data[,col]), breaks = 50, main = paste("Histogram of ", col), xlab = col)
}
```

Hence, we will propose the following model to include all the covariates:
```{r}
m3 <- lm(log(crmrte) ~ prbarr + prbconv + prbpris + avgsen + log(polpc) + density + taxpc + west + central + urban + pctmin80 + log(wcon)
         + log(wtuc) + log(wtrd) + log(wfir) + log(wser) + log(wmfg) + log(wfed) + log(wsta) + log(wloc) + log(mix) + pctymle, data=data)
```

```{r}
plot(m3)
bptest(m3)
```

violation of zero-conditional mean & homoskedasticity, potentially due to lack of data points in the negative and postive directions. 


```{r}
vif(m3)
```

Looking at the variance inflation factors, we don't see any variable with high (>10) vif, meaning that we don't have high multi-collinearity in our model.

```{r}
# To address heteroskedasticity, we use robust standard errors.
coeftest(m3, vcov = vcovHC)
```

```{r}
#compute standard errors for model 3 for regression table
(se.m3 = sqrt(diag(vcovHC(m3))))

# We pass the standard errors into stargazer through 
# the se argument.
stargazer(m3, type = "text", omit.stat = "f",
          se = list(se.m3),
          star.cutoffs = c(0.05, 0.01, 0.001))
``` 

According to the hypothesis tests using the heteroskedasticity-robust standard errors, we can see that variables xxx, xxx... show strong statistical significance. And according to the slope coefficients, we have ceteris paribus interpretation of xx, xxx, which shows strong pratical significance.   

# Summary of Models

# Discussion of Causality

# Conclusion
>>>>>>> 7fd17e4bc07a980522a0ed9f96e0e16808776ec9

# References:
"Shattering "Broken Windows": An Analysis of San Francisco’s Alternative Crime Policies", CENTER ON JUVENILE AND CRIMINAL JUSTICE, October 1999 http://www.cjcj.org/uploads/cjcj/documents/shattering.pdf
